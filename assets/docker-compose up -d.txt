docker-compose up -d

docker cp "C:\Users\Ayoub Maimmadi\Documents\AUI\Tajedine Rachidi\ETL and ELT integration pipelines\sales_xlsx" namenode:/tmp/sales_xlsx

docker cp "C:\Users\Ayoub\Documents\AUI_Files\Tajedine Rachidi\ETL-and-ELT-integration-pipelines\sales_csv" namenode:/tmp/sales_csv

docker exec -it namenode bash

hdfs dfs -mkdir /sales_data

hdfs dfs -rm -r /sales_data

hdfs dfs -put /tmp/s/* /sales_data

hdfs dfs -ls /sales_data

cd /etc/apt/apt.conf.d/

echo 'Acquire::Check-Valid-Until "false";' | tee 99no-check-valid-until

echo "" > /etc/apt/sources.list

echo "deb http://archive.debian.org/debian/ stretch main contrib non-free" >> /etc/apt/sources.list
echo "deb-src http://archive.debian.org/debian/ stretch main contrib non-free" >> /etc/apt/sources.list
echo "deb http://archive.debian.org/debian-security/ stretch/updates main contrib non-free" >> /etc/apt/sources.list
echo "deb-src http://archive.debian.org/debian-security/ stretch/updates main contrib non-free" >> /etc/apt/sources.list

apt-get update

apt-get install -y python3

python3 --version

----
docker exec -it namenode /bin/bash
docker exec -it datanode /bin/bash
docker exec -it nodemanager /bin/bash
docker exec -it historyserver /bin/bash
docker exec -it resourcemanager /bin/bash


docker exec -it namenode ls /

docker cp "C:\Users\Ayoub Maimmadi\Documents\AUI\Tajedine Rachidi\ETL and ELT integration pipelines\mapper.py" namenode:/home/hadoop/mapper.py

docker exec -it namenode chmod +x /home/hadoop/mapper.py

ls /tmp/sales_csv
ls /tmp/sales_xlsx

hdfs dfs -mkdir /sales_data
hdfs dfs -put /tmp/sales_csv/* /sales_data

rm /home/mapper.py

docker cp "C:\Users\Ayoub\Documents\AUI_Files\Tajedine Rachidi\ETL-and-ELT-integration-pipelines\mapper.py" namenode:/home/mapper.py

hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
-files /home/mapper.py -mapper "/usr/bin/python3 /home/mapper.py" \hdfs dfs -put /tmp/s/* /sales_data
-input /sales_data/* -output /output_results

hdfs dfs -rm -r /output_results

hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -mapper "python /home/mapper.py" \
    -input /sales_data \
    -output /output_results

/opt/hadoop-3.2.1/bin/hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \
    -files /home/mapper.py \
    -mapper mapper.py \
    -input /sales_data \
    -output /output_results



%HADOOP_HOME%\sbin\start-dfs.cmd
http://localhost:9870/dfshealth.html#tab-overview.
%HADOOP_HOME%\sbin\start-yarn.cmd
http://localhost:8088
hadoop fs -ls /
C:\Users\LENOVO\ETL-and-ELT-integration-pipelines\sales_csv
hadoop fs -copyFromLocal C:\Users\LENOVO\ETL-and-ELT-integration-pipelines\sales_csv /sales_data


C:\Users\LENOVO>hadoop fs -put C:\path\to\your\local\sales_records_n1.csv /sales_data
C:\Users\LENOVO>hadoop fs -put C:\path\to\your\local\sales_records_n2.csv /sales_data
C:\Users\LENOVO>hadoop fs -put C:\path\to\your\local\sales_records_n3.csv /sales_data
C:\Users\LENOVO>hadoop fs -put C:\path\to\your\local\sales_records_n4.csv /sales_data
C:\Users\LENOVO>hadoop fs -put C:\path\to\your\local\sales_records_n5.csv /sales_data
hadoop fs -put C:\Users\LENOVO\ETL-and-ELT-integration-pipelines\mapper.py /python



hadoop jar /path/to/hadoop-streaming.jar \
-file /python/mapper.py -mapper "python mapper.py" \
-input /sales_data/* -output /output-sales-data
